{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "cell_execution_strategy": "setup",
      "authorship_tag": "ABX9TyPfdxDLezglde3oASBmDLDQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/noambassat/SupremeCourtClassifier/blob/main/LLM_Court.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "vvVtg-CzCG2y"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import joblib\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import drive\n",
        "import time\n",
        "import re\n",
        "import openai\n",
        "import json\n",
        "from sklearn.model_selection import train_test_split\n",
        "import ast"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def fix_hebrew_labels(ax):\n",
        "    \"\"\" Reverse Hebrew labels in Matplotlib plots. \"\"\"\n",
        "    for label in ax.get_xticklabels() + ax.get_yticklabels():\n",
        "        text = label.get_text()\n",
        "        if any(\"\\u0590\" <= char <= \"\\u05FF\" for char in text):  # Detect Hebrew characters\n",
        "            label.set_text(text[::-1])\n",
        "    ax.figure.canvas.draw()\n"
      ],
      "metadata": {
        "id": "V-qebznTV7Sm"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rUu4l0zkFPox",
        "outputId": "9fbbea9f-57ef-479a-b0a5-f387db532a22"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "folder_id = \"בית המשפט - קבצים שנבדקו\"\n",
        "directory_path = f\"/content/drive/My Drive/{folder_id}\"\n",
        "classifiers_path = \"/content/drive/MyDrive/בית המשפט - מסווגים/\"\n",
        "\n",
        "if not os.path.exists(directory_path):\n",
        "    print(f\"Directory {directory_path} does not exist. Please check the folder path.\")\n",
        "else:\n",
        "    dataframes = []\n",
        "\n",
        "    for file_name in os.listdir(directory_path):\n",
        "        if file_name.endswith(\".xlsx\") or file_name.endswith(\".xls\"):\n",
        "            file_path = os.path.join(directory_path, file_name)\n",
        "            try:\n",
        "                df = pd.read_excel(file_path)\n",
        "                dataframes.append(df)\n",
        "            except Exception as e:\n",
        "                print(f\"Failed to read {file_name}: {e}\")\n",
        "\n",
        "    if dataframes:\n",
        "        combined_df = pd.concat(dataframes, ignore_index=True)\n",
        "        print(\"All Excel files have been concatenated successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7-8rCVEjDjXf",
        "outputId": "e2895f01-30cb-461d-82d6-4623e74f638d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/openpyxl/worksheet/_reader.py:329: UserWarning: Data Validation extension is not supported and will be removed\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.11/dist-packages/openpyxl/worksheet/_reader.py:329: UserWarning: Data Validation extension is not supported and will be removed\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.11/dist-packages/openpyxl/worksheet/_reader.py:329: UserWarning: Data Validation extension is not supported and will be removed\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.11/dist-packages/openpyxl/worksheet/_reader.py:329: UserWarning: Data Validation extension is not supported and will be removed\n",
            "  warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All Excel files have been concatenated successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "full_ra_rap = pd.read_excel(\"/content/drive/My Drive/full_ra_rap.xlsx\")"
      ],
      "metadata": {
        "id": "ysV94PcW_8Ol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "check_gender_bias = pd.concat([dataframes[0], dataframes[1], dataframes[2]])\n",
        "check_gender_bias = pd.merge(full_ra_rap, check_gender_bias, on='מספר הליך', how='left')\n",
        "check_gender_bias.drop_duplicates(subset = 'מספר הליך',inplace = True)\n",
        "check_gender_bias.dropna(subset = [\"בקשה לרשות ערעור התקבלה?\"],inplace=True)"
      ],
      "metadata": {
        "id": "ZsdLraC9_0iM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for col in check_gender_bias.columns:\n",
        "    if(col.find(\"מגדר\")!=-1 or col.find(\"ערעור\")!=-1 or col.find(\"מספר \")!=-1):\n",
        "      print(col)"
      ],
      "metadata": {
        "id": "FljLeZkhB5yE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "check_gender_bias = check_gender_bias[[\"מספר הליך\",\"מספר השופטים\",\"בקשה לרשות ערעור התקבלה?\",\"מגדר\"]]"
      ],
      "metadata": {
        "id": "h8EkRHnENJTS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "check_gender_bias[[\"בקשה לרשות ערעור התקבלה?\",\"מגדר\"]]"
      ],
      "metadata": {
        "id": "tczUJbPLCHcA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from ast import literal_eval\n",
        "\n",
        "\n",
        "# המרת המחרוזת ברשימת מגדרים למבנה רשימה\n",
        "check_gender_bias[\"מגדר\"] = check_gender_bias[\"מגדר\"].apply(literal_eval)  # ממיר טקסט לרשימה\n",
        "\n",
        "# פונקציה להשטחת הנתונים – כל שופט יקבל שורה משלו\n",
        "def expand_genders(df):\n",
        "    expanded_data = []\n",
        "    for _, row in df.iterrows():\n",
        "        for gender in row[\"מגדר\"]:\n",
        "            expanded_data.append({\"Gender\": gender, \"Appeal Outcome\": row[\"בקשה לרשות ערעור התקבלה?\"]})\n",
        "    return pd.DataFrame(expanded_data)\n",
        "\n",
        "# הרחבת הטבלה\n",
        "df_expanded = expand_genders(check_gender_bias)\n",
        "\n",
        "# ספירת תוצאות הערעור לפי מגדר\n",
        "outcome_counts = df_expanded.groupby([\"Gender\", \"Appeal Outcome\"]).size().unstack().fillna(0)\n",
        "\n",
        "# יצירת גרף התפלגות תוצאות לפי מגדר\n",
        "plt.figure(figsize=(8, 5))\n",
        "outcome_counts.plot(kind=\"bar\", stacked=True)\n",
        "plt.ylabel(\"מספר ערעורים\")\n",
        "plt.title(\"התפלגות תוצאות הערעור לפי מגדר השופט/ת\")\n",
        "plt.xticks(rotation=0)\n",
        "plt.legend(title=\"תוצאת הערעור\")\n",
        "plt.show()\n",
        "\n",
        "# חישוב יחס קבלת ערעור לכל מגדר\n",
        "outcome_counts[\"Total\"] = outcome_counts.sum(axis=1)\n",
        "outcome_counts[\"Acceptance Rate\"] = outcome_counts.get(\"התקבל\", 0) / outcome_counts[\"Total\"]\n",
        "\n",
        "# יצירת גרף של אחוזי קבלת ערעור לפי מגדר\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.barplot(x=outcome_counts.index, y=outcome_counts[\"Acceptance Rate\"])\n",
        "plt.ylabel(\"שיעור קבלת ערעור\")\n",
        "plt.title(\"שיעור קבלת ערעור לפי מגדר השופט/ת\")\n",
        "plt.ylim(0, 1)\n",
        "plt.show()\n",
        "\n",
        "# הצגת הטבלה עם התוצאות\n",
        "print(outcome_counts)\n"
      ],
      "metadata": {
        "id": "UiCOhdOnDn7H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oD0_v0cYHx7W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "NBZadrvUE0bf"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oWEkpygnFz8R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bias - number of Judges"
      ],
      "metadata": {
        "id": "n4HEGAa7HeQZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from ast import literal_eval\n",
        "\n",
        "# טעינת הנתונים\n",
        "df = check_gender_bias.copy()\n",
        "\n",
        "# חישוב מספר השופטים בכל תיק\n",
        "df[\"מספר שופטים\"] = df[\"מגדר\"].apply(len)\n",
        "\n",
        "# חלוקת המקרים לשתי קבוצות: שופט יחיד או יותר מאחד\n",
        "df[\"יותר משופט אחד\"] = df[\"מספר שופטים\"] > 1\n",
        "\n",
        "# ספירת המקרים לפי מספר השופטים והאם הערעור התקבל\n",
        "outcome_counts = df.groupby(\"יותר משופט אחד\")[\"בקשה לרשות ערעור התקבלה?\"].value_counts(normalize=True).unstack()\n",
        "\n",
        "# יצירת גרף\n",
        "plt.figure(figsize=(8, 5))\n",
        "outcome_counts.plot(kind=\"bar\", stacked=True)\n",
        "plt.ylabel(\"שיעור הערעורים (%)\")\n",
        "plt.title(\"השפעת מספר השופטים על קבלת הערעור\")\n",
        "plt.xticks([0, 1], [\"שופט אחד\", \"יותר משופט אחד\"], rotation=0)\n",
        "plt.legend(title=\"תוצאת הערעור\")\n",
        "plt.show()\n",
        "\n",
        "# חישוב אחוז קבלת ערעור לפי מספר השופטים\n",
        "acceptance_rate = outcome_counts.get(\"התקבל\", 0)\n",
        "\n",
        "# הדפסת סטטיסטיקה\n",
        "print(\"שיעור קבלת הערעור לפי מספר שופטים:\")\n",
        "print(acceptance_rate)\n"
      ],
      "metadata": {
        "id": "tuLNzfE5BBhg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import ast\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy.stats import chi2_contingency\n",
        "\n",
        "df = check_gender_bias.copy()\n",
        "\n",
        "# לוודא שהעמודה 'מגדר' היא רשימה ולא מחרוזת\n",
        "df[\"מגדר\"] = df[\"מגדר\"].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
        "\n",
        "# הוספת עמודת מספר השופטים בכל מקרה\n",
        "df[\"מספר שופטים\"] = df[\"מגדר\"].apply(len)\n",
        "\n",
        "# יצירת עמודת אינדיקטור אם הערעור התקבל או לא\n",
        "df[\"ערעור התקבל\"] = df[\"בקשה לרשות ערעור התקבלה?\"].apply(lambda x: 1 if x == \"התקבל\" else 0)\n",
        "\n",
        "### בדיקת הקשר בין מספר השופטים להחלטה ###\n",
        "grouped_by_judges = df.groupby(\"מספר שופטים\")[\"ערעור התקבל\"].mean()\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.barplot(x=grouped_by_judges.index, y=grouped_by_judges.values, palette=\"Blues_r\")\n",
        "plt.xlabel(\"מספר שופטים\")\n",
        "plt.ylabel(\"שיעור קבלת ערעור\")\n",
        "plt.title(\"השפעת מספר השופטים על קבלת הערעור\")\n",
        "plt.show()\n",
        "\n",
        "# טבלת שכיחות\n",
        "judge_count_table = df.groupby(\"מספר שופטים\")[\"ערעור התקבל\"].value_counts().unstack()\n",
        "print(judge_count_table)\n",
        "\n",
        "# בדיקה סטטיסטית - האם יש קשר מובהק בין מספר שופטים להחלטה\n",
        "chi2, p, _, _ = chi2_contingency(judge_count_table.fillna(0))\n",
        "print(f\"Chi-Square Test: p-value = {p:.4f}\")\n",
        "if p < 0.05:\n",
        "    print(\"מסקנה: קיים קשר מובהק סטטיסטית בין מספר השופטים לקבלת הערעור.\")\n",
        "else:\n",
        "    print(\"מסקנה: אין קשר מובהק סטטיסטית בין מספר השופטים לקבלת הערעור.\")\n",
        "\n",
        "### בדיקת ביאס מגדרי ###\n",
        "# חישוב אחוז קבלת ערעור לפי מגדר\n",
        "def get_gender_stats(gender_list, outcome):\n",
        "    return any(g in gender_list for g in outcome)\n",
        "\n",
        "df[\"מכיל זכר\"] = df[\"מגדר\"].apply(lambda x: \"זכר\" in x)\n",
        "df[\"מכיל נקבה\"] = df[\"מגדר\"].apply(lambda x: \"נקבה\" in x)\n",
        "\n",
        "# בדיקת הבדל בין מקרים עם לפחות שופטת לבין אלו ללא\n",
        "gender_bias_table = df.groupby(\"מכיל נקבה\")[\"ערעור התקבל\"].value_counts().unstack()\n",
        "print(\"טבלת שכיחות לפי נוכחות שופטות:\\n\", gender_bias_table)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.barplot(x=gender_bias_table.index, y=gender_bias_table[1] / (gender_bias_table[1] + gender_bias_table[0]), palette=\"Reds_r\")\n",
        "plt.xlabel(\"האם יש לפחות שופטת?\")\n",
        "plt.ylabel(\"שיעור קבלת ערעור\")\n",
        "plt.title(\"השפעת נוכחות שופטת על קבלת הערעור\")\n",
        "plt.show()\n",
        "\n",
        "# בדיקה סטטיסטית - האם יש הבדל מובהק\n",
        "chi2, p, _, _ = chi2_contingency(gender_bias_table.fillna(0))\n",
        "print(f\"Chi-Square Test for Gender Bias: p-value = {p:.4f}\")\n",
        "if p < 0.05:\n",
        "    print(\"מסקנה: יש עדות מובהקת לכך שנוכחות שופטות משפיעה על החלטת הערעור.\")\n",
        "else:\n",
        "    print(\"מסקנה: אין עדות מובהקת לכך שנוכחות שופטות משפיעה על החלטת הערעור.\")\n"
      ],
      "metadata": {
        "id": "FMphMwfMI3Sg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(dataframes)"
      ],
      "metadata": {
        "id": "cPRCkVSd61os"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_1 = dataframes[0].copy()\n",
        "\n",
        "df_1.columns"
      ],
      "metadata": {
        "id": "5Rv2ZUDm8Dk6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_1 = df_1[['גוף המסמך', 'מספר הליך', 'שם הליך',\n",
        "       'שנת פתיחת ההליך', 'גוף המסמך חתוך',  'בקשה לרשות ערעור התקבלה?',\n",
        "       'טקסט שמצביע על תוצאת בקשה שונה', 'הערעור התקבל?',\n",
        "       'רע\"פ בקשה אחרת או דלמטה מיוחד', 'הערות ',\n",
        "       'הערות קידוד חוזר']]"
      ],
      "metadata": {
        "id": "DY_3qlUc-I43"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_2 = dataframes[1].copy()"
      ],
      "metadata": {
        "id": "uawB3EtE-t8N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_2 = df_2[['גוף המסמך', 'מספר הליך', 'שם הליך',\n",
        "       'שנת פתיחת ההליך', 'גוף המסמך חתוך',\n",
        "       'בקשה לרשות ערעור התקבלה?',\n",
        "       'טקסט שמצביע על תוצאת בקשה שונה (טקסטים שונים מופרדים ב-**)',\n",
        "       'הערעור התקבל?', 'רע\"פ בקשה אחרת או דלמטה מיוחד',\n",
        "       'הערות תוצאת ערעור', 'הערות ', 'הערות קידוד חוזר']]"
      ],
      "metadata": {
        "id": "JXniQQU3-2Gz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.concat([df_1, df_2], ignore_index=True)"
      ],
      "metadata": {
        "id": "z0NIHaoM_lDF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_3 = dataframes[2].copy()"
      ],
      "metadata": {
        "id": "GTp1f9y2_G3l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_3 = df_3[['גוף המסמך', 'מספר הליך', 'שם הליך',\n",
        "       'שנת פתיחת ההליך',\n",
        "       'בקשה לרשות ערעור התקבלה?',\n",
        "       'טקסט שמצביע על תוצאת בקשה שונה (טקסטים שונים מופרדים ב-**)',\n",
        "       'הערעור התקבל?', 'הערות תוצאת ערעור', 'רע\"פ בקשה אחרת או דלמטה מיוחד', 'הערות ']]"
      ],
      "metadata": {
        "id": "KY4yog2-_usH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.concat([df, df_3], ignore_index=True)"
      ],
      "metadata": {
        "id": "VVsLDIhj_G71"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zmxYCBBC_B2h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "columns_to_check = [\n",
        "    \"הערות \",\n",
        "    \"הערות תוצאת ערעור\",\n",
        "    \"הערות קידוד חוזר\",\n",
        "    \"טקסט שמצביע על תוצאת בקשה שונה\",\n",
        "    \"טקסט שמצביע על תוצאת בקשה שונה (טקסטים שונים מופרדים ב-**)\",\n",
        "    \"טקסט שמצביע על תוצאת בקשה שונה (טקסטים שונים מופרדים ב-**)\"\n",
        "]\n",
        "\n",
        "\n",
        "df = df[~df[columns_to_check].isna().all(axis=1)]"
      ],
      "metadata": {
        "id": "HYPwg-TeAoWD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "yBNrZ9ST4YK_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jQd6gOcv_C0u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IQTmuq1v_c1J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SKArgzv3_lC5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(df[\"גוף המסמך\"])\n",
        "\n",
        "df[\"length\"] = df[\"גוף המסמך\"].apply(lambda x: len(str(x).split()))\n",
        "\n",
        "average_length = df[\"length\"].mean()\n",
        "max_length = df[\"length\"].max()\n",
        "min_length = df[\"length\"].min()\n",
        "\n",
        "print(f\"אורך ממוצע של פסקי דין: {average_length:.2f} מילים\")\n",
        "print(f\"פסק הדין הארוך ביותר מכיל {max_length} מילים\")\n",
        "print(f\"פסק הדין הקצר ביותר מכיל {min_length} מילים\")\n"
      ],
      "metadata": {
        "id": "KEkZ-UDy4gbv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_list_to_string(text):\n",
        "    if isinstance(text, str) and text.startswith(\"[\") and text.endswith(\"]\"):\n",
        "        try:\n",
        "            text_list = ast.literal_eval(text)\n",
        "            return ' '.join(text_list)\n",
        "        except (ValueError, SyntaxError):\n",
        "            return text\n",
        "    return text\n",
        "\n",
        "def clean_text(text):\n",
        "    if isinstance(text, list):\n",
        "        text = ' '.join(text)\n",
        "    elif isinstance(text, str):\n",
        "        text = re.sub(r'\\n+', ' ', text)  # הסרת שורות חדשות מרובות\n",
        "        text = re.sub(r'\\n', '', text)   # הסרת תווי newline \\n מהטקסט\n",
        "        text = re.sub(r'\\xa0', ' ', text)  # הסרת תווי \\xa0 מהטקסט\n",
        "        text = re.sub(r'\\s+', ' ', text)  # הסרת רווחים מרובים\n",
        "\n",
        "        text = re.sub(r\"העותק כפוף לשינויי עריכה וניסוח.*?$\", \"\", text, flags=re.MULTILINE)\n",
        "        text = re.sub(r\"מרכז מידע, טל' \\d{2,3}-\\d{6,7}.*?$\", \"\", text, flags=re.MULTILINE)\n",
        "        text = re.sub(r\"אתר אינטרנט, .*?$\", \"\", text, flags=re.MULTILINE)\n",
        "    return text\n",
        "\n",
        "print(\"שורות לא קריאות לפני המרה:\")\n",
        "print(df[\"גוף המסמך\"].head())\n",
        "\n",
        "df[\"גוף המסמך\"] = df[\"גוף המסמך\"].apply(clean_text)\n",
        "\n",
        "print(\"\\nשורות לאחר המרה:\")\n",
        "print(df[\"גוף המסמך\"].head())\n",
        "\n",
        "file_path_full = 'checked_df_cleaned.csv' # FULL RAP DATA CLEANED\n",
        "df.to_csv(file_path_full, index=False, encoding='utf-8')\n"
      ],
      "metadata": {
        "id": "lLLLzAUR6XvO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_relevant_part(text):\n",
        "    words = str(text).split()\n",
        "    if len(words) > 1000:\n",
        "        return \" \".join(words[-800:-20])\n",
        "    return text\n",
        "\n",
        "df[\"גוף המסמך חתוך\"] = df[\"גוף המסמך\"].apply(extract_relevant_part)\n",
        "\n",
        "df[[\"גוף המסמך\", \"גוף המסמך חתוך\"]].head()\n"
      ],
      "metadata": {
        "id": "997LkOwE5bcg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yI75UNYF-8eD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df[['גוף המסמך', 'מספר הליך', 'שם הליך', 'שנת פתיחת ההליך',\n",
        "       'גוף המסמך חתוך', 'בקשה לרשות ערעור התקבלה?',\n",
        "       'טקסט שמצביע על תוצאת בקשה שונה', 'הערעור התקבל?',\n",
        "       'רע\"פ בקשה אחרת או דלמטה מיוחד', 'הערות ', 'הערות קידוד חוזר',\n",
        "       'טקסט שמצביע על תוצאת בקשה שונה (טקסטים שונים מופרדים ב-**)',\n",
        "       'הערות תוצאת ערעור', 'length',]]"
      ],
      "metadata": {
        "id": "2SuWC1ad7GuW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns = df.columns.str.strip()\n",
        "df.columns = df.columns.str.replace(\"\\n\", \"\")\n",
        "df.columns = df.columns.str.replace(\"  \", \" \")"
      ],
      "metadata": {
        "id": "yAKcKxV--dai"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.rename(columns={\n",
        "    'הערות ':\"הערות\",\n",
        "    'טקסט שמצביע על תוצאת בקשה שונה':\"טקסט שמצביע על תוצאת בקשה\",\n",
        "    \"טקסט שמצביע על תוצאת בקשה שונה (טקסטים שונים מופרדים ב-**)\": \"טקסט שמצביע על התוצאה\"\n",
        "}, inplace=True)\n",
        "\n",
        "df.columns\n"
      ],
      "metadata": {
        "id": "kdd6KMu89uSk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "columns_to_merge = [\n",
        "    'טקסט שמצביע על תוצאת בקשה',\n",
        "    'רע\"פ בקשה אחרת או דלמטה מיוחד',  \"הערות\", 'הערות קידוד חוזר',\n",
        "    \"טקסט שמצביע על התוצאה\",\n",
        "    'הערות תוצאת ערעור'\n",
        "]\n",
        "\n",
        "def clean_merge_columns(row, columns):\n",
        "    values = []\n",
        "    for col in columns:\n",
        "        if pd.notna(row[col]) and str(row[col]).strip():\n",
        "            text = f\"{str(row[col]).strip()}\"\n",
        "            text.replace(\"?\", \"\")\n",
        "            text\n",
        "            text = re.sub(r\"הערה של \\S+\\s*\", \"\", text)\n",
        "            values.append(text)\n",
        "\n",
        "    return \"\\n\".join(values) if values else None\n",
        "\n",
        "# יצירת עמודה מאוחדת עם כותרות לכל ערך\n",
        "df[\"תוצאה מסכמת\"] = df.apply(lambda row: clean_merge_columns(row, columns_to_merge), axis=1)\n",
        "\n",
        "\n",
        "# הצגת מספר דוגמאות לבדיקה\n",
        "df[[\"תוצאה מסכמת\"]].head()\n"
      ],
      "metadata": {
        "id": "SIeNQC_W8IBy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for i in range(3):\n",
        "    print(f\"פסק דין {i+1} (תוצאה מסכמת):\\n\")\n",
        "    print(df[\"תוצאה מסכמת\"].iloc[i])\n",
        "    print(\"=\"*80)\n"
      ],
      "metadata": {
        "id": "bqBlYGtW7B_u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "df[[\"גוף המסמך חתוך\", \"תוצאה מסכמת\"]].sample(5, random_state=42)"
      ],
      "metadata": {
        "id": "3sAWoccf8nIb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[[\"גוף המסמך חתוך\", \"תוצאה מסכמת\"]].sample(5, random_state=42)"
      ],
      "metadata": {
        "id": "mIGeDoOU8nNx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_excel(\"/content/dataframe_cleaned.xlsx\", index=False)"
      ],
      "metadata": {
        "id": "sWN2TicK8nF6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# split - train, validation, test"
      ],
      "metadata": {
        "id": "DvkoVz9nl7qx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, temp_data = train_test_split(df, test_size=0.2, random_state=42)\n",
        "valid_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42)\n",
        "\n",
        "def convert_to_jsonl(data, filename):\n",
        "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
        "        for _, row in data.iterrows():\n",
        "            json.dump({\n",
        "                \"messages\": [\n",
        "                    {\"role\": \"system\", \"content\": \"אתה עוזר משפטי לניתוח פסקי דין מסוג רע\\\"פ\"},\n",
        "                    {\"role\": \"user\", \"content\": row[\"גוף המסמך חתוך\"]},\n",
        "                    {\"role\": \"assistant\", \"content\": row[\"תוצאה מסכמת\"]}\n",
        "                ]\n",
        "            }, f, ensure_ascii=False)\n",
        "            f.write(\"\\n\")\n",
        "\n",
        "convert_to_jsonl(train_data, \"train.jsonl\")\n",
        "convert_to_jsonl(valid_data, \"valid.jsonl\")\n",
        "convert_to_jsonl(test_data, \"test.jsonl\")\n"
      ],
      "metadata": {
        "id": "-bvQXAHkypbm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine tune Llama 2 using LoRA"
      ],
      "metadata": {
        "id": "trxAYL0pM2fe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from huggingface_hub import login\n",
        "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "# import json\n",
        "# from peft import LoraConfig, get_peft_model, TaskType\n",
        "# from transformers import TrainingArguments, Trainer"
      ],
      "metadata": {
        "id": "QA46zLkn0dE7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# with open(\"/content/Llama_login.txt\", \"r\") as file:\n",
        "#   llama_login = file.read().strip()\n",
        "\n",
        "# login(llama_login)\n",
        "\n",
        "# MODEL_NAME = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "\n",
        "# tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_auth_token=True)\n",
        "# model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, torch_dtype=\"auto\", device_map=\"auto\", use_auth_token=True)\n",
        "\n",
        "# print(\"🎯 המודל נטען בהצלחה!\")\n"
      ],
      "metadata": {
        "id": "e9nziHo7OYqg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def generate_summary_llama2(case_text):\n",
        "#     input_ids = tokenizer(case_text, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
        "#     output = model.generate(input_ids, max_new_tokens=100)\n",
        "#     return tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "# # בודקים פסק דין לדוגמה\n",
        "# sample_case = dataset[\"valid\"][0][\"messages\"][1][\"content\"]\n",
        "# llama2_summary = generate_summary_llama2(sample_case)\n",
        "\n",
        "# print(llama2_summary)\n"
      ],
      "metadata": {
        "id": "P1nOtQI51LNd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aZWwNNhE4k95"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test_text = \"פסק דין משפטי לדוגמה על סכסוך אזרחי\"\n",
        "# input_ids = tokenizer(test_text, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
        "\n",
        "# output = model.generate(input_ids, max_new_tokens=100)\n",
        "# generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "# print(\"פלט המודל ללא Fine-Tuning:\\n\", generated_text)\n"
      ],
      "metadata": {
        "id": "S8GBbJ7m3mTS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def generate_summary_llama2(case_text):\n",
        "#     prompt = (\n",
        "#         \"סכם את פסק הדין הבא בצורה תמציתית:\\n\\n\"\n",
        "#         f\"{case_text}\\n\\n\"\n",
        "#         \"סיכום משפטי: \"\n",
        "#     )\n",
        "#     input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
        "#     output = model.generate(input_ids, max_new_tokens=100)\n",
        "#     return tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "\n",
        "# llama2_summary = generate_summary_llama2(sample_case)\n",
        "# print(\"סיכום משופר על ידי Llama 2:\", llama2_summary)\n"
      ],
      "metadata": {
        "id": "xim3_RSeMz7d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GPT Fine tuning"
      ],
      "metadata": {
        "id": "sQs5YfJclw-_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# with open(\"/content/api_key.txt\", \"r\") as file:\n",
        "#     api_key = file.read().strip()\n",
        "\n",
        "# client = openai.OpenAI(api_key=api_key)\n",
        "\n",
        "# train_file = client.files.create(\n",
        "#     file=open(\"train.jsonl\", \"rb\"),\n",
        "#     purpose=\"fine-tune\"\n",
        "# )\n",
        "\n",
        "# print(\"Train File ID:\", train_file.id)"
      ],
      "metadata": {
        "id": "aIVvqpF7B5j-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# files = client.files.list()\n",
        "# print(files)"
      ],
      "metadata": {
        "id": "wv5HKJP8QAWN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fine_tune = client.fine_tuning.jobs.create(\n",
        "#     training_file=train_file.id,\n",
        "#     model=\"gpt-3.5-turbo\"\n",
        "# )\n",
        "\n",
        "# print(\"Fine-Tuning Job ID:\", fine_tune.id)"
      ],
      "metadata": {
        "id": "surRCAvEQEYC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fine_tune_status = client.fine_tuning.jobs.retrieve(fine_tune.id)\n",
        "# print(fine_tune_status)"
      ],
      "metadata": {
        "id": "8qqWR0MtQPb-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# job_id = fine_tune_status.id\n",
        "# while True:\n",
        "#     fine_tune_job = client.fine_tuning.jobs.retrieve(job_id)\n",
        "#     print(f\"Status: {fine_tune_job.status}\")\n",
        "\n",
        "#     if fine_tune_job.status in [\"succeeded\", \"failed\", \"cancelled\"]:\n",
        "#         print(f\"Fine-Tuned Model ID: {fine_tune_job.fine_tuned_model}\")\n",
        "#         break\n",
        "\n",
        "#     time.sleep(60*5)"
      ],
      "metadata": {
        "id": "GVzAVhHfQ-nS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# print(fine_tune_job.trained_tokens)\n",
        "# print(fine_tune_job.fine_tuned_model)"
      ],
      "metadata": {
        "id": "MKax7TTrQqsn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Save the fine-tuned model ID in a text file (only needs to be done once)\n",
        "# with open(\"/content/fine_tuned_model.txt\", \"w\") as file:\n",
        "#     file.write(fine_tune_job.fine_tuned_model)\n"
      ],
      "metadata": {
        "id": "pF6oKBjFtPkg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# result_files = client.files.list()\n",
        "# for file in result_files.data:\n",
        "#     if \"fine-tuning\" in file.purpose:\n",
        "#         print(file.filename, file.id)"
      ],
      "metadata": {
        "id": "Y75Vbz6sQxLf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# response = client.chat.completions.create(\n",
        "#     model=fine_tune_job.fine_tuned_model,\n",
        "#     messages=[\n",
        "#         {\"role\": \"system\", \"content\": 'אתה מומחה משפטי לניתוח ותמצות שורה תחתונה של פסקי דין מסוג רע\"פ'},\n",
        "#         {\"role\": \"user\", \"content\": test_data.iloc[0][\"גוף המסמך חתוך\"]}\n",
        "#     ]\n",
        "# )\n",
        "\n",
        "# print(\"תשובה שהמודל נתן:\")\n",
        "# print(response.choices[0].message.content)\n",
        "\n",
        "# print(\"תוצאה מקורית:\\n\")\n",
        "# print(test_data.iloc[0][\"תוצאה מסכמת\"])\n"
      ],
      "metadata": {
        "id": "aRWCn-FFnE61"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for i in range(len(test_data)):\n",
        "#   response = client.chat.completions.create(\n",
        "#     model=fine_tune_job.fine_tuned_model,\n",
        "#     messages=[\n",
        "#         {\"role\": \"system\", \"content\": 'אתה מומחה משפטי לניתוח ותמצות שורה תחתונה של פסקי דין מסוג רע\"פ'},\n",
        "#         {\"role\": \"user\", \"content\": test_data.iloc[i][\"גוף המסמך חתוך\"]}\n",
        "#     ]\n",
        "#   )\n",
        "#   print(f\"מסמך מספר {i+1}:\")\n",
        "#   print(\"פסק דין:\")\n",
        "#   print(test_data.iloc[i][\"גוף המסמך חתוך\"])\n",
        "\n",
        "#   print(\"תשובה שהמודל נתן:\")\n",
        "#   print(response.choices[0].message.content)\n",
        "\n",
        "#   print(\"תוצאה מקורית:\")\n",
        "#   print(test_data.iloc[i][\"תוצאה מסכמת\"])\n",
        "\n",
        "#   print(\"-------------------------------\")"
      ],
      "metadata": {
        "id": "QyoNDWY3xL9A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GPT - Evaluation"
      ],
      "metadata": {
        "id": "thNQeKO5sGpn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.merge(df, check_gender_bias, on=\"מספר הליך\", how=\"left\", suffixes=('', '_dup'))\n",
        "\n",
        "df = df.loc[:, ~df.columns.str.endswith('_dup')]\n"
      ],
      "metadata": {
        "id": "jknPah_8NoFc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "IIvYn9i7P1MO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import json\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.stats as stats\n",
        "import ast\n",
        "\n",
        "\n",
        "with open(\"/content/api_key.txt\", \"r\") as file:\n",
        "    api_key = file.read().strip()\n",
        "\n",
        "client = openai.OpenAI(api_key=api_key)\n"
      ],
      "metadata": {
        "id": "DazRrJnLS5Z_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Fetch all fine-tuned models\n",
        "fine_tuned_models = client.fine_tuning.jobs.list()\n",
        "\n",
        "# Extract the most recent fine-tuned model ID\n",
        "latest_model = None\n",
        "for model in fine_tuned_models:\n",
        "    if model.status == \"succeeded\":  # Making sure it's fully trained\n",
        "        latest_model = model.fine_tuned_model\n",
        "        break\n",
        "\n",
        "if latest_model:\n",
        "    print(\"Using Fine-Tuned Model\")\n",
        "else:\n",
        "    print(\"No fine-tuned model found.\")\n"
      ],
      "metadata": {
        "id": "rN8x3pMhtaEF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Ensure 'מגדר' column is properly formatted as a list\n",
        "def safe_eval(value):\n",
        "    \"\"\" Convert string representation of a list to an actual list safely. \"\"\"\n",
        "    if isinstance(value, str):\n",
        "        try:\n",
        "            return ast.literal_eval(value)  # Convert string to list\n",
        "        except (SyntaxError, ValueError):\n",
        "            return None  # If conversion fails, return None\n",
        "    elif isinstance(value, list):\n",
        "        return value\n",
        "    else:\n",
        "        return None  # If it's NaN or other type, return None\n",
        "\n",
        "df[\"מגדר\"] = df[\"מגדר\"].apply(safe_eval)\n",
        "\n",
        "# Drop rows where we don't have judge information\n",
        "df = df[df[\"מגדר\"].notna()]\n",
        "\n",
        "# Create a new column for number of judges\n",
        "df[\"num_judges\"] = df[\"מגדר\"].apply(len)\n",
        "\n",
        "\n",
        "df = df[df[\"num_judges\"] > 0]\n",
        "\n",
        "\n",
        "# Count occurrences of appeal acceptance based on number of judges\n",
        "real_counts = df.groupby(\"num_judges\")[\"בקשה לרשות ערעור התקבלה?\"].value_counts().unstack().fillna(0)\n",
        "\n",
        "# Plot real data\n",
        "plt.figure(figsize=(10, 6))\n",
        "real_counts.div(real_counts.sum(axis=1), axis=0).plot(kind=\"bar\", stacked=True, colormap=\"Reds_r\")\n",
        "plt.title(\"Actual Appeal Acceptance Rate by Number of Judges\")\n",
        "plt.xlabel(\"Number of Judges\")\n",
        "plt.ylabel(\"Proportion\")\n",
        "plt.xticks(rotation=0)\n",
        "plt.legend(title=\"Appeal Outcome\")\n",
        "plt.show()\n",
        "\n",
        "# Chi-Square test to check statistical significance\n",
        "chi2, p_value, _, _ = stats.chi2_contingency(real_counts)\n",
        "print(f\"Chi-Square Test: p-value = {p_value:.4f}\")\n",
        "\n",
        "# Interpretation\n",
        "if p_value > 0.05:\n",
        "    print(\"No statistically significant relationship between the number of judges and appeal acceptance.\")\n",
        "else:\n",
        "    print(\"A statistically significant relationship exists between the number of judges and appeal acceptance.\")\n"
      ],
      "metadata": {
        "id": "9uoYNHxKN_ce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AnjBS-jpTVV7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to ask GPT to explain its reasoning\n",
        "def gpt_explain_patterns(case_text):\n",
        "    response = client.chat.completions.create(\n",
        "        model=latest_model,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\":\"אתה מומחה משפטי. הסבר את הנימוקים מאחורי ההחלטה במקרה הנתון.\"},\n",
        "            {\"role\": \"user\", \"content\": f\"Case: {case_text}\\n\\nמהם הגורמים המרכזיים המשפיעים על ההחלטה?\"}\n",
        "        ]\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "# Apply GPT explanation function\n",
        "df[\"model_patterns\"] = df[\"בקשה לרשות ערעור התקבלה?\"].apply(lambda x: gpt_explain_patterns(x))\n",
        "\n",
        "# Display examples of patterns extracted by GPT\n",
        "print(df[[\"בקשה לרשות ערעור התקבלה?\", \"model_patterns\"]].head())\n"
      ],
      "metadata": {
        "id": "tC1vY02iSr0P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import lime\n",
        "import lime.lime_text\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import openai\n",
        "\n",
        "\n",
        "# Prepare dataset for analysis\n",
        "X_text = df[\"בקשה לרשות ערעור התקבלה?\"].astype(str)\n",
        "\n",
        "# Create a simple TF-IDF model to analyze textual patterns\n",
        "vectorizer = TfidfVectorizer()\n",
        "X_tfidf = vectorizer.fit_transform(X_text)\n",
        "\n",
        "# Train a simple classifier to act as a proxy for GPT\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "clf = LogisticRegression()\n",
        "clf.fit(X_tfidf, df[\"בקשה לרשות ערעור התקבלה?\"])\n",
        "\n",
        "# Use LIME to explain what influences GPT's decisions\n",
        "explainer = lime.lime_text.LimeTextExplainer(class_names=[\"Rejected\", \"Accepted\"])\n",
        "idx = 0  # Choose an index to explain\n",
        "\n",
        "exp = explainer.explain_instance(\n",
        "    X_text.iloc[idx],\n",
        "    clf.predict_proba,\n",
        "    num_features=10\n",
        ")\n",
        "\n",
        "# Show LIME explanation\n",
        "exp.show_in_notebook()\n"
      ],
      "metadata": {
        "id": "HMcQif70NoOd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load test dataset\n",
        "def load_jsonl(filename):\n",
        "    with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
        "        return [json.loads(line) for line in f]\n",
        "\n",
        "test_data = load_jsonl(\"test.jsonl\")"
      ],
      "metadata": {
        "id": "geq-TedN50BA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fairness"
      ],
      "metadata": {
        "id": "dkOKjhBEuWWU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Function to evaluate factuality (Faithfulness)\n",
        "def evaluate_factuality(original_text, generated_summary):\n",
        "    response = client.chat.completions.create(\n",
        "        model=latest_model,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": 'אתה מומחה משפטי. דרג את מידת הנאמנות של התקציר לפסק הדין המקורי בסולם של 1-5, כאשר 5 הוא הנאמנות הגבוהה ביותר.'},\n",
        "            {\"role\": \"user\", \"content\": f\"פסק דין: {original_text}\\n\\nסיכום: {generated_summary}\\n\\nעד כמה הסיכום מדויק ונאמן למקור?\"}\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "# Evaluate on test data\n",
        "for i, case in enumerate(test_data[:5]):  # Running on the first 5 cases\n",
        "    original_text = case[\"messages\"][1][\"content\"]\n",
        "    generated_summary = case[\"messages\"][2][\"content\"]\n",
        "\n",
        "    faithfulness_score = evaluate_factuality(original_text, generated_summary)\n",
        "\n",
        "    print(f\"Case {i+1}:\")\n",
        "    print(\"Original Judgment:\", original_text)\n",
        "    print(\"Generated Summary:\", generated_summary)\n",
        "    print(\"Faithfulness Score:\", faithfulness_score)\n",
        "    print(\"-------------------------------------------------\")\n"
      ],
      "metadata": {
        "id": "zCHXUvfh7nbb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "המודל לא מוסיף או משנה מידע קריטי.\n",
        "\n",
        "שיפור אפשרי -\n",
        "1. הוספת PENALTY למודל על השמטה של מידע מהותי. למשל אם הערעור מתקבל חלקית והמודל טוען שנדחה לגמרי.\n",
        "2. חיבור לRAG - כדי למנוע עוד טעויות (מאגר משפטי נוסף)"
      ],
      "metadata": {
        "id": "mOR-g9qg6dux"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bias"
      ],
      "metadata": {
        "id": "avLNRRMguxps"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shap\n",
        "\n",
        "# Function to check SHAP (explainability)\n",
        "def explain_shap(case_text):\n",
        "    explainer = shap.Explainer(client.chat.completions.create)\n",
        "    shap_values = explainer([case_text])\n",
        "    return shap_values\n",
        "\n",
        "# Function to check Counterfactual Fairness\n",
        "def check_bias(input_text, changed_text):\n",
        "    response_base = client.chat.completions.create(\n",
        "        model=latest_model,\n",
        "        messages=[{\"role\": \"user\", \"content\": input_text}]\n",
        "    )\n",
        "\n",
        "    response_changed = client.chat.completions.create(\n",
        "        model=latest_model,\n",
        "        messages=[{\"role\": \"user\", \"content\": changed_text}]\n",
        "    )\n",
        "\n",
        "    return response_base.choices[0].message.content == response_changed.choices[0].message.content\n",
        "\n",
        "# Running Bias & Fairness Checks on Test Data\n",
        "for i, case in enumerate(test_data[:5]):\n",
        "    original_text = case[\"messages\"][1][\"content\"]\n",
        "    modified_text = original_text.replace(\"השופט\", \"השופטת\")  # Small gender swap for bias testing\n",
        "\n",
        "    bias_detected = check_bias(original_text, modified_text)\n",
        "\n",
        "    print(f\"Case {i+1}:\")\n",
        "    print(\"Original Judgment:\", original_text)\n",
        "    print(\"Modified Judgment:\", modified_text)\n",
        "    print(f\"Bias Detected (Counterfactual Fairness Check): {bias_detected}\")\n",
        "    print(\"-------------------------------------------------\")\n"
      ],
      "metadata": {
        "id": "xtz_uNBou0JI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ב4 מתוך 5 מקרים לא היתה הטיה מגדרית."
      ],
      "metadata": {
        "id": "Gz0jnUCNzBpZ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "l7jmE17j8jXp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Function to check bias\n",
        "def check_bias(model_name, input_text, changed_feature):\n",
        "    response1 = client.chat.completions.create(\n",
        "        model=model_name,\n",
        "        messages=[{\"role\": \"system\", \"content\": \"אתה מומחה משפטי לניתוח פסקי דין\"},\n",
        "                  {\"role\": \"user\", \"content\": input_text}]\n",
        "    )\n",
        "\n",
        "    response2 = client.chat.completions.create(\n",
        "        model=model_name,\n",
        "        messages=[{\"role\": \"system\", \"content\": \"אתה מומחה משפטי לניתוח פסקי דין\"},\n",
        "                  {\"role\": \"user\", \"content\": changed_feature}]\n",
        "    )\n",
        "\n",
        "    return response1.choices[0].message.content == response2.choices[0].message.content\n",
        "\n",
        "input_text = \"השופט גבריאל פסל את הערעור.\"\n",
        "changed_text = \"השופטת גבריאלה פסלה את הערעור.\"\n",
        "\n",
        "bias_detected = check_bias(latest_model, input_text, changed_text)\n",
        "print(f\"Bias Detected: {bias_detected}\")\n"
      ],
      "metadata": {
        "id": "zlnapDQNzM8i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### בדיקה אם יש Bias בטקסטים המקוריים"
      ],
      "metadata": {
        "id": "4whyfl-s3VZs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def check_bias_in_original_data(df):\n",
        "    male_terms = [\"השופט\", \"התובע\", \"הנאשם\", \"העותר\"]\n",
        "    female_terms = [\"השופטת\", \"התובעת\", \"הנאשמת\", \"העותרת\"]\n",
        "\n",
        "    male_count = sum(df[\"גוף המסמך חתוך\"].str.count('|'.join(male_terms)))\n",
        "    female_count = sum(df[\"גוף המסמך חתוך\"].str.count('|'.join(female_terms)))\n",
        "\n",
        "    return {\"Male Count\": male_count, \"Female Count\": female_count, \"Bias Ratio\": male_count / (female_count + 1)}\n",
        "\n",
        "bias_in_data = check_bias_in_original_data(df)\n",
        "print(\"Bias in Original Data:\", bias_in_data)\n"
      ],
      "metadata": {
        "id": "vV1cHqHP3TOj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "יש הטייה בנתוני המקור.\n",
        "1. Data Augmentation\n",
        "2. UnderSamling\n",
        "3. Regulariation"
      ],
      "metadata": {
        "id": "pICzSzDu8F0h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### האם המודל מחמיר את ההטיה הקיימת"
      ],
      "metadata": {
        "id": "rCuMwzHU3d0e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def check_bias_in_model(model_name, input_text_male, input_text_female):\n",
        "    response_male = client.chat.completions.create(\n",
        "        model=model_name,\n",
        "        messages=[{\"role\": \"system\", \"content\": \"סכם את פסק הדין בקצרה.\"},\n",
        "                  {\"role\": \"user\", \"content\": input_text_male}]\n",
        "    ).choices[0].message.content\n",
        "\n",
        "    response_female = client.chat.completions.create(\n",
        "        model=model_name,\n",
        "        messages=[{\"role\": \"system\", \"content\": \"סכם את פסק הדין בקצרה.\"},\n",
        "                  {\"role\": \"user\", \"content\": input_text_female}]\n",
        "    ).choices[0].message.content\n",
        "\n",
        "    return {\"Male Output\": response_male, \"Female Output\": response_female}\n",
        "\n",
        "bias_in_model = check_bias_in_model(latest_model, input_text, changed_text)\n",
        "print(\"Bias in Model:\", bias_in_model)\n"
      ],
      "metadata": {
        "id": "-1CPmfqk05Br"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "מסקנה -\n",
        "\n",
        "אין הטיה מגדרית מבחינת מונחים כלליים (השופט - השופטת)\n",
        "\n",
        "אבל - יש השפעה של שמות ספציפיים מה שמרמז על BIAS בנתונים המקוריים.\n"
      ],
      "metadata": {
        "id": "lzTU1goY9uGW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "-KFwRZTO9-B2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### FactScore\n",
        "מדד לשימור משמעות משפטית"
      ],
      "metadata": {
        "id": "XOE5hiVp05cE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_factscore_smart(model_name, original_text, generated_summary):\n",
        "    response = client.chat.completions.create(\n",
        "        model=model_name,\n",
        "        messages=[{\"role\": \"system\", \"content\": \"אתה מומחה משפטי. האם הסיכום הבא תואם להחלטת פסק הדין? התייחס למשמעות המשפטית ולא רק לניסוח מילולי.\"},\n",
        "                  {\"role\": \"user\", \"content\": f\"טקסט מקורי: {original_text}\\nסיכום: {generated_summary}\\nהאם הסיכום נאמן למשמעות של פסק הדין?\"}]\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "factscore_result = evaluate_factscore_smart(latest_model, original_text, generated_summary)\n",
        "print(f\"FactScore Evaluation (Smart): {factscore_result}\")\n"
      ],
      "metadata": {
        "id": "giUhDlvN09aT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "original_text"
      ],
      "metadata": {
        "id": "CeoBOONa4FJy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generated_summary"
      ],
      "metadata": {
        "id": "1IrXohx74IVG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "תוצאה - המודל משמר את המשמעות המשפטית של ההחלטה המקורית ונאמן למקור\n",
        "\n",
        "שיפורים לבדיקות\n",
        "1.   Edge cases\n",
        "\n"
      ],
      "metadata": {
        "id": "cd__qsKG7dOm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Attributable Generation"
      ],
      "metadata": {
        "id": "PrSerzS71H8A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "בדיקת בעיית Hallucination"
      ],
      "metadata": {
        "id": "XN2klCnj7vf2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def check_attributable_generation_smart(model_name, original_text, generated_summary):\n",
        "    response = client.chat.completions.create(\n",
        "        model=model_name,\n",
        "        messages=[{\"role\": \"system\", \"content\": \"האם הסיכום הבא מתבסס על החלטות משפטיות קודמות, או שהוא מכיל מידע מומצא שלא נתמך בטקסט?\"},\n",
        "                  {\"role\": \"user\", \"content\": f\"טקסט מקורי: {original_text}\\nסיכום: {generated_summary}\\nהאם הסיכום מתבסס על מידע משפטי קיים?\"}]\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "attribution_score = check_attributable_generation_smart(latest_model, original_text, generated_summary)\n",
        "print(f\"Attributable Generation Score (Smart): {attribution_score}\")\n"
      ],
      "metadata": {
        "id": "iCGcg2G81FVM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "5wlY7xo471vR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "KSgLnZfjzHLV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Promt engineering vs Fine tuning"
      ],
      "metadata": {
        "id": "1g7Slv5uvaup"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Function for Prompt Engineering Approach\n",
        "def prompt_engineering_example(query):\n",
        "    engineered_prompt = f\"אתה מומחה משפטי. סכם את פסק הדין הבא בקצרה:\\n\\n{query}\"\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo-0125\",\n",
        "        messages=[{\"role\": \"user\", \"content\": engineered_prompt}]\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "# Function for Fine-Tuning Approach\n",
        "def fine_tuning_example(query):\n",
        "    response = client.chat.completions.create(\n",
        "        model= latest_model,\n",
        "        messages=[{\"role\": \"user\", \"content\": query}]\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "# Running the test on real cases\n",
        "for i, case in enumerate(test_data[:5]):\n",
        "    original_text = case[\"messages\"][1][\"content\"]\n",
        "\n",
        "    prompt_result = prompt_engineering_example(original_text)\n",
        "    fine_tuned_result = fine_tuning_example(original_text)\n",
        "\n",
        "    print(f\"Case {i+1}:\")\n",
        "    print(\"Original Judgment:\", original_text)\n",
        "    print(\"\\n📌 Prompt Engineering Result:\", prompt_result)\n",
        "    print(\"\\n📌 Fine-Tuning Result:\", fine_tuned_result)\n",
        "    print(\"-------------------------------------------------\")\n"
      ],
      "metadata": {
        "id": "9FQ2gbaJvhXM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Compare models"
      ],
      "metadata": {
        "id": "DjKF2l4C7n6J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil"
      ],
      "metadata": {
        "id": "F-B6-wfaGJts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/api_key.txt\", \"r\") as file:\n",
        "    api_key = file.read().strip()\n",
        "\n",
        "client = openai.OpenAI(api_key=api_key)\n",
        "\n",
        "train_file = client.files.create(\n",
        "    file=open(\"train.jsonl\", \"rb\"),\n",
        "    purpose=\"fine-tune\"\n",
        ")\n",
        "\n",
        "# print(\"Train File ID:\", train_file.id)"
      ],
      "metadata": {
        "id": "B_3rtPbxB6HH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def generate_summary_gpt3(case_text):\n",
        "    response = client.chat.completions.create(\n",
        "        model=lates_model,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"אתה עוזר משפטי המסכם פסקי דין.\"},\n",
        "            {\"role\": \"user\", \"content\": case_text}\n",
        "        ]\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n"
      ],
      "metadata": {
        "id": "VkQN9Vr38TEL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_summary_llama2(case_text):\n",
        "    prompt = (\n",
        "        f\"{case_text}\\n\\n\"\n",
        "        \"### Task: Summarize the court ruling in one sentence.\\n\"\n",
        "        \"Summary:\"\n",
        "    )\n",
        "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
        "    output = model.generate(input_ids, max_new_tokens=50)\n",
        "    return tokenizer.decode(output[0], skip_special_tokens=True).strip()\n"
      ],
      "metadata": {
        "id": "njGD9Try9TVd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from rouge import Rouge\n",
        "\n",
        "# 📌 פונקציה להשוואת ביצועים בין המודלים\n",
        "def compare_models(original_text, human_summary):\n",
        "    gpt_summary = generate_summary_gpt3(original_text)  #  GPT-3.5\n",
        "    llama_summary = generate_summary_llama2(original_text)  #  Llama 2\n",
        "\n",
        "    bleu_gpt = sentence_bleu([human_summary.split()], gpt_summary.split())\n",
        "    bleu_llama = sentence_bleu([human_summary.split()], llama_summary.split())\n",
        "\n",
        "    rouge = Rouge()\n",
        "    rouge_gpt = rouge.get_scores(gpt_summary, human_summary)[0][\"rouge-l\"][\"f\"]\n",
        "    rouge_llama = rouge.get_scores(llama_summary, human_summary)[0][\"rouge-l\"][\"f\"]\n",
        "\n",
        "    print(f\"📌 BLEU GPT-3.5: {bleu_gpt:.4f}, BLEU Llama-2: {bleu_llama:.4f}\")\n",
        "    print(f\"📌 ROUGE GPT-3.5: {rouge_gpt:.4f}, ROUGE Llama-2: {rouge_llama:.4f}\")\n",
        "\n",
        "    return gpt_summary, llama_summary\n",
        "\n",
        "# 📌 הרצת השוואה לדוגמה\n",
        "original_case = dataset[\"valid\"][0][\"messages\"][1][\"content\"]\n",
        "human_summary = dataset[\"valid\"][0][\"messages\"][2][\"content\"]\n",
        "\n",
        "gpt_output, llama_output = compare_models(original_case, human_summary)\n",
        "\n",
        "print( gpt_output)\n",
        "print(llama_output)\n"
      ],
      "metadata": {
        "id": "LcYa9cSaCXXn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}